{
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "ml1-arm64",
      "language": "python",
      "name": "ml1-arm64"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "nteract": {
      "version": "0.7.1"
    },
    "colab": {
      "name": "LinReg_Workshop.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-qjsO3-CyTH"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBBWZsb3CyTO"
      },
      "source": [
        "## A.1: Making the data\n",
        "\n",
        "We'll first construct a synthetic data set..using a function from the `scikit-learn` library. Synthetic data is nice in the sense that we can constrain how the noise behaves, and thus isolate effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDBQLvGDCyTP"
      },
      "source": [
        "%matplotlib inline\n",
        "from sklearn.datasets import make_regression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoCW2nSyCyTR"
      },
      "source": [
        "This data is generated from the canonical generating process assumed for linear regression: a gaussian distribution centered at the regression line on the y axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ4g4sN9CyTR"
      },
      "source": [
        "#code adapted from http://tillbergmann.com/blog/python-gradient-descent.html\n",
        "X, y, coef = make_regression(n_samples = 100, \n",
        "                       n_features=1, \n",
        "                       noise=20,\n",
        "                       random_state=2017,\n",
        "                       bias=0.0,\n",
        "                       coef=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q98EPmtRCyTS"
      },
      "source": [
        "Notice that the X is in the canonical array-of-arrays format.\n",
        "**Try and print its shape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNpyFGTeCyTS"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij_miKEiCyTU"
      },
      "source": [
        "We are fitting a model with an intercept. Lets see what it is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5STse-DGCyTU"
      },
      "source": [
        "coef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRSyktEDCyTV"
      },
      "source": [
        "We can plot the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f34COrzrCyTV"
      },
      "source": [
        "plt.plot(X,y, 'o');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBwzhUNYCyTW"
      },
      "source": [
        "For the purposes of drawing the regression line, lets create a uniform grid of points, and then reshape it into the canonical format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmedjoqJCyTX"
      },
      "source": [
        "xgrid = np.linspace(-2.5,2.5,1000)\n",
        "Xgrid = xgrid.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qx4TklhCyTY"
      },
      "source": [
        "## A.2: Fit using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecdZyQdpCyTY"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN8FEHXqCyTY"
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(X,y) # fit the model with the existing data\n",
        "ypgrid = lr.predict(Xgrid) # now predict it on the grid\n",
        "lr.coef_, lr.intercept_ # get the slope and the intercept"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihlAru1ACyTY"
      },
      "source": [
        "Notice that the slope and the intercept are not what we fed into the model, but close. This is because the model fitted depends on the exact way points were generated.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xThEkz-FCyTZ"
      },
      "source": [
        "plt.plot(Xgrid, ypgrid)\n",
        "plt.plot(X, y, '.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vs9Qhf2CyTa"
      },
      "source": [
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGmhfmAjCyTa"
      },
      "source": [
        "r2_score(y, lr.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeErYPb9CyTi"
      },
      "source": [
        "## A.3: Train vs Test Split\n",
        "\n",
        "A grid like the one we created might contain some of the points we fit this model on. This is called **Data Contamination** and is a big no-no. If we want an independent estimate of the error, we should hold out some points in a test set. Then we want to guarantee that there is no overlap between the initial sample, or **training set**, and the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxJH20BmCyTi"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBRKTAcGCyTi"
      },
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=2022)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASm-rgK_CyTi"
      },
      "source": [
        "Now lets fit the model on the training set and evaluate it both on the training set and the test set. We print the R^2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IqnGc1iCyTi"
      },
      "source": [
        "lr2 = LinearRegression().fit(Xtrain, ytrain)\n",
        "r2_test = r2_score(ytest, lr2.predict(Xtest))\n",
        "r2_train = r2_score(ytrain, lr2.predict(Xtrain))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_tnvbNfCyTj"
      },
      "source": [
        "\"Train R2 is {}, while test R^2 is {}\".format(r2_train, r2_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B: Multi-Linear Regression on the ads dataset\n",
        "\n",
        "Let us first import the dataset."
      ],
      "metadata": {
        "id": "Xf-rHnL6ja2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from prettytable import PrettyTable"
      ],
      "metadata": {
        "id": "ba96mvWAj3ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the dataset:"
      ],
      "metadata": {
        "id": "m0jllfBOkmrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the file \"Advertising.csv\"\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/univai-ghf/ghfmedia/main/data/Regression/Advertising.csv\")"
      ],
      "metadata": {
        "id": "Ehya3zXikoiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a quick look at the data to list all the predictors\n",
        "df.head()"
      ],
      "metadata": {
        "id": "97Yx7jSJkp69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>CODING BREAKOUT</h3>"
      ],
      "metadata": {
        "id": "qMb1pmYBK9hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Breakout, you need to fit a Linear Regression model on the Advertising Data, where X will store your predictors (TV, Radio and Newspaper) and y stores the response variable (Sales). Do not split the data into Train and test set for this exercise."
      ],
      "metadata": {
        "id": "m9gp_WXpMPMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store the predictors and response variables in X and y respecively"
      ],
      "metadata": {
        "id": "83nMeuETNn2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "ZLc85F7vK_Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the LinearRegression Model on the Dataset.When calling the LinearRegression constructor, pass the fit_intercept argument as False(just for this exercise). Also, print the coef_ learned by the model."
      ],
      "metadata": {
        "id": "cG3bmH3iN8Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "vqn-UK51LtA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional Exercise) As seen in the slides, there exists a closed form solution for calculating the value of coefficients for Linear Regression which is given as follows:<br>\n",
        "<center> <h3>$\\beta$ = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y</h3></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "rcdF6W3sRble"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try plugging in the values of X and y in the formula to compare the beta (coefficient) values with what you obtained from sklearn.<br>\n",
        "**Hints** <br>\n",
        "To perform Transpose Operation on a Numpy ndarray/Pandas DataFrame X, you can simply do X.T<br>\n",
        "To perform Matrix inversion, you can use the np.linalg.inv() method.<br>\n",
        "To multiply 2 Matrices X and Y, you can simply do X @ Y. Remember both the matrices should be of appropiate order to peform Matrix Multiplication."
      ],
      "metadata": {
        "id": "qZMjII5aTrDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "o5XPfGphLsq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***Breakout Ends here*** "
      ],
      "metadata": {
        "id": "EZc6Dyvw6FSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now run multi-linear regression with Sales as the reponse variable Y. We will actually do so for all possible sets of predictor variables: from only considering individual TV, Radio, Newspaper values to all of them taken together. This is to illustrate how our R2 scores can change based on how many predictor variables we have."
      ],
      "metadata": {
        "id": "M6tugSoeqafn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#List to store the r2score values\n",
        "r2_list = []\n",
        "\n",
        "#List of all predictor combinations to fit the curve\n",
        "cols = [['TV'],['Radio'],['Newspaper'],['TV','Radio'],['TV','Newspaper'],['Radio','Newspaper'],['TV','Radio','Newspaper']]\n",
        "\n",
        "for i in cols:\n",
        "    #Set each of the predictors from the previous list as x\n",
        "    x_ads = df[i].to_numpy()\n",
        "    \n",
        "    \n",
        "    #\"Sales\" column is the reponse variable\n",
        "    y_ads = df[\"Sales\"].to_numpy()\n",
        "    \n",
        "   \n",
        "    #Splitting the data into train-test sets with 80% training data and 20% testing data. \n",
        "    #Set random_state as 0\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_ads, y_ads, test_size=0.2, random_state=2022)\n",
        "\n",
        "    #Create a LinearRegression object and fit the model\n",
        "    lr3 = LinearRegression()\n",
        "    lr3 = LinearRegression().fit(x_train, y_train) # fit the model on train data\n",
        "    r2_test = r2_score(y_test, lr3.predict(x_test)) # compute the score on test data\n",
        "    \n",
        "    #Append the R2Score to the list\n",
        "    r2_list.append(r2_test)\n"
      ],
      "metadata": {
        "id": "Ds0FbCcwmNy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = PrettyTable(['Predictors', 'R2Scores'])\n",
        "\n",
        "#Loop to display the predictor combinations along with the R2 score of the corresponding model\n",
        "for i in range(len(r2_list)):\n",
        "    t.add_row([cols[i],r2_list[i]])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "9-wMJzOTnsfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C: Multilinear regression on California Housing Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "5ChqSkQXy3mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The California Housing Dataset consists of the following columns"
      ],
      "metadata": {
        "id": "RqIWcrNYVVAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **longitude**: A measure of how far west a house is; a higher value is farther west\n",
        "\n",
        "2. **latitude**: A measure of how far north a house is; a higher value is farther north\n",
        "\n",
        "3. **housingMedianAge**: Median age of a house within a block; a lower number is a newer building\n",
        "\n",
        "4. **totalRooms**: Total number of rooms within a block\n",
        "\n",
        "5. **totalBedrooms**: Total number of bedrooms within a block\n",
        "\n",
        "6. **population**: Total number of people residing within a block\n",
        "\n",
        "7. **households**: Total number of households, a group of people residing within a home unit, for a block\n",
        "\n",
        "8. **medianIncome**: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
        "\n",
        "9. **medianHouseValue**: Median house value for households within a block (measured in US Dollars)\n",
        "\n",
        "10. **oceanProximity**: Location of the house w.r.t ocean/sea"
      ],
      "metadata": {
        "id": "8PQx3I1SVBrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the dataset:"
      ],
      "metadata": {
        "id": "F7asdBDow7uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the file \"housing.csv\"\n",
        "df_housing = pd.read_csv(\"https://raw.githubusercontent.com/univai-ghf/ghfmedia/main/data/Regression/housing.csv\")"
      ],
      "metadata": {
        "id": "rUD1njwJcdPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a quick look at the data to list all the predictors\n",
        "df_housing.head()"
      ],
      "metadata": {
        "id": "CkZnTLn9c812"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a quick look at the datatypes of the predicators\n",
        "df_housing.dtypes"
      ],
      "metadata": {
        "id": "8M4qMTbXc8r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for Null/Empty Values:"
      ],
      "metadata": {
        "id": "lFXVxf1Mxsj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# isnull() function returns True if that element is missing in the dataframe, False otherwise\n",
        "df_housing.isnull()"
      ],
      "metadata": {
        "id": "OyI6mdelgxYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sum function to detect which columns have missing elements\n",
        "df_housing.isnull().sum()"
      ],
      "metadata": {
        "id": "TL3sJQhcymMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that the feature total_bedrooms has 207 NULL/empty values, these need to be dealt with before applying any model"
      ],
      "metadata": {
        "id": "zc5Ptn2EzQLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for the median of total_bedrooms column\n",
        "\n",
        "df_housing.total_bedrooms.median()"
      ],
      "metadata": {
        "id": "_x48FSd1iKVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the empty values with the median of that column is one of the common ways to deal with it\n",
        "\n",
        "df_housing.total_bedrooms=df_housing.total_bedrooms.fillna(df_housing.total_bedrooms.median())"
      ],
      "metadata": {
        "id": "EIPsEV67iOl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check to be sure whether the missing values are dealt with.\n",
        "\n",
        "df_housing.isnull().sum()"
      ],
      "metadata": {
        "id": "O9an7Qc_hHjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with Categorcial Features"
      ],
      "metadata": {
        "id": "gsqbFB0r1JrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning models in general can only deal with numeric values as input, so to deal with categorical variables, we need to encode them into some numberic representation. One of the most commonly used methodology is **One Hot Encoding**."
      ],
      "metadata": {
        "id": "sxPG997y1RrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Before Applying One Hot Encoding:**\n",
        "\n",
        "<table width=\"200px\">\n",
        "  <tr>\n",
        "    <th>Color</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Red</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Blue</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Green</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Red</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>\n",
        "\n",
        "**After Applying One Hot Encoding:**\n",
        "<table width=\"400px\">\n",
        "  <tr>\n",
        "    <th>Color_Red</th>\n",
        "    <th>Color_Blue</th>\n",
        "    <th>Color_Green</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    \n",
        "  </tr>\n",
        "  <tr>\n",
        "      <td>0</td>\n",
        "    <td>1</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "     <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "     <td>1</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "PbuBIBdZ2qn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pd.get_dummies() function to one hot encode the categorical variable \"ocean_proximity\"\n",
        "\n",
        "df_housing=pd.get_dummies(df_housing,columns=['ocean_proximity'])\n",
        "df_housing.head()"
      ],
      "metadata": {
        "id": "_WwX9p5Ldz-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What scale to use?<br>\n",
        "For many regression problems the numerical range of values could lead to wild coefficients. For instance, the CA longitude values are all in a tiny range from [-122.2x to -122.2y] so the differences are only in the second decimal place! To mitigate such issues, one often scales the entires using some standard transformations (e.g., subtract mean, scale by std. deviation). Here we will put all entries in the interval [0,1] by subtracting the min and dividing by the range.\n"
      ],
      "metadata": {
        "id": "I8S3n1xjbNM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be observed, the range of values the predictors can vary a lot,which can make it difficult to interpret the coefficients of the model. So its a good practice to Standardize the data between 0 and 1. For this we make use of of sklearn's MinMaxScaler<br>\n",
        "MinMax Scaling is achieved using the following formula:<br>\n",
        "<center> $\\frac{(X-X.min)}{(X.max-X.min)}$</center>\n",
        "where X is one of the predictors in the dataFrame\n"
      ],
      "metadata": {
        "id": "smgDZQPl4B1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "WY4mYJrM22bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the dataset using MinMaxScaler,it returns a numpy ndarray\n",
        "# Convert it back to Pandas DataFrame, making it convinient to split it into predictors and response variables later\n",
        "\n",
        "df_housing_scaled=MinMaxScaler().fit_transform(df_housing)\n",
        "df_housing_scaled=pd.DataFrame(df_housing_scaled,columns=df_housing.columns)"
      ],
      "metadata": {
        "id": "eibrqYIO0LmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing_scaled.head()"
      ],
      "metadata": {
        "id": "BcxEVYUr0Le8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>CODING BREAKOUT</h3>"
      ],
      "metadata": {
        "id": "ptyvZK8v3A8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "df_housing_scaled contains the Pandas DataFrame for California Housing Dataset, you need to seperately store the predictor variables and response variable in X and y respectively."
      ],
      "metadata": {
        "id": "awsmH9ng3zjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "DBCAJk4mfFyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the Data into train and test set, with test_size=20% and random_state=2022 to reproduce the results"
      ],
      "metadata": {
        "id": "356L27gg4jGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "xbeK38SIfFkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the shapes of X_train, y_train, X_test and y_test"
      ],
      "metadata": {
        "id": "1nGUfpHT46vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "-6_pa3h7jm9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the Linear Regression Model on Training Data"
      ],
      "metadata": {
        "id": "xZj0IDc95Ee1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "iaqXb3Rdjmw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the Coefficients of the model"
      ],
      "metadata": {
        "id": "kOsxID0Tq4y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "WTRmmLQJq2tl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute R<sup>2</sup> for training and test set"
      ],
      "metadata": {
        "id": "yDWDEtI46Sng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "XEByTd3XkSid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakout Ends here**"
      ],
      "metadata": {
        "id": "PojserE38ZAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Things to Remember before fitting a Model**<br>\n",
        "\n",
        "\n",
        "1.   Always check for NULL values in the dataset, these empty values cause issues when fitting a model and need to be dealt with before hand. Usual approach to deal with numeric values is to replace the the NULL value with Mean/Median of the column, whereas for categorical data, the simplest approach is to eliminate the entire corrosponding row.<br> **Note:** There are multiple approaches to deal with NULL values and is not limited to the approaches mentioned, it depends on various factors such as task to be solved, availability of data etc. \n",
        "2.   Check for Categorical variables in the dataset, if present, deal with them using techniques such as One Hot Encoding. Remember sometimes categories can be Numbers such as Category 1,2,3 etc. We should treat them as categories and deal with them accordingly.\n",
        "3. Its also a good practice to Scale/Normalize your predictors especially when there is lot of variation between variables. Some standard approaches to scale your predictors are MinMaxScaling, StandardScalar etc.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rxbcINucoN0E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7g78BarCyTb"
      },
      "source": [
        "## D: Model Confidence\n",
        "\n",
        "We'll sample 20 points from the data set. We do this by sampling 20 indices, index into X and y, and then fit on the sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ajLCrOWCyTb"
      },
      "source": [
        "sample_indices = np.random.choice(range(100), size=20, replace=False)\n",
        "sample_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code adapted from http://tillbergmann.com/blog/python-gradient-descent.html\n",
        "X, y, coef = make_regression(n_samples = 100, \n",
        "                       n_features=1, \n",
        "                       noise=20,\n",
        "                       random_state=2017,\n",
        "                       bias=0.0,\n",
        "                       coef=True)"
      ],
      "metadata": {
        "id": "okGg2AREy_Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qjIde5_CyTb"
      },
      "source": [
        "We create a sample by using the sample indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3fp5XwTCyTc"
      },
      "source": [
        "Xsample = X[sample_indices]\n",
        "ysample = y[sample_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE3G5P4iCyTc"
      },
      "source": [
        "**Find the $R^2$ score of a fit to this sample, on this sample**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8M1Da1CyTc"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKo99KksCyTd"
      },
      "source": [
        "Lets check the sensitivity of our prediction to our sample. We'll do this 1000 times..that is we'll sample a new set of 20 points, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyD960zACyTe"
      },
      "source": [
        "scores = []\n",
        "models=[]\n",
        "for i in range(1000):\n",
        "    sample_indices = np.random.choice(range(100), size=20, replace=False)\n",
        "    Xsample = X[sample_indices]\n",
        "    ysample = y[sample_indices]\n",
        "    m = LinearRegression().fit(Xsample, ysample)\n",
        "    models.append(m)\n",
        "    scores.append(m.score(Xsample, ysample))\n",
        "plt.hist(scores,  bins=np.linspace(0.7, 1, 30))\n",
        "plt.xlim(0.7,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAqEUbtvCyTe"
      },
      "source": [
        "Let us check the slope and intercepts fitted on the different samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMn-JQLkCyTe"
      },
      "source": [
        "plt.hist([models[i].coef_[0] for i in range(1000)], bins=10);\n",
        "plt.xlim([60, 100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teZnVAyICyTf"
      },
      "source": [
        "plt.hist([models[i].intercept_ for i in range(100)], bins=10);\n",
        "plt.xlim([-15, 10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2CoX-EhL7_iy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}